{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "# BeautifulSoup is used for web scraping\n",
    "from bs4 import BeautifulSoup  \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_google(api_key, cse_id, query, start_page):\n",
    "    # Format the current date to get the past 24 hours' news\n",
    "    current_time = datetime.now()\n",
    "    past_24_hours = current_time - timedelta(days=1)\n",
    "    date_restrict = past_24_hours.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'cx': cse_id,\n",
    "        'q': query,\n",
    "        'dateRestrict': f'd1',  # Restricts results to the past 24 hours\n",
    "        'start': start_page\n",
    "    }\n",
    "    response = requests.get(\"https://www.googleapis.com/customsearch/v1\", params=params)\n",
    "    return response.json()\n",
    "\n",
    "def extract_content_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # This is a simplified way of extracting main text, might need customization based on the website structure\n",
    "        text = ' '.join([p.text for p in soup.find_all('p')])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting content from {url}: {e}\")\n",
    "        return \"\"\n",
    "    \n",
    "def extract_information(result):\n",
    "    data = []\n",
    "    for item in result['items']:\n",
    "        title = item['title']\n",
    "        link = item['link']\n",
    "        content = extract_content_from_url(link)  # Extracting the full content from the link\n",
    "        data.append({'title': title, 'link': link, 'content': content})\n",
    "    return data\n",
    "\n",
    "def collect_data(api_key, cse_id, query, pages=1):\n",
    "    all_data = []\n",
    "    for page in range(1, pages + 1):\n",
    "        start_page = (page - 1) * 10 + 1\n",
    "        results = search_google(api_key, cse_id, query, start_page)\n",
    "        data = extract_information(results)\n",
    "        all_data.extend(data)\n",
    "        if page < pages:\n",
    "            time.sleep(2)  # Delay between requests to avoid hitting rate limits\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve API key and CSE ID from environment variables\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "cse_id = os.getenv('GOOGLE_CSE_ID')\n",
    "\n",
    "query = \"Copper\"\n",
    "# keywords = [\"copper\"]  # Keywords for relevance filtering\n",
    "\n",
    "# Collecting data from the top 5 pages\n",
    "collected_data = collect_data(api_key, cse_id, query, 5)\n",
    "\n",
    "# Save the collected data to a JSON file\n",
    "with open('copper.json', 'w') as f:\n",
    "    json.dump(collected_data, f, indent=4)\n",
    "\n",
    "print(f\"Collected {len(collected_data)} relevant articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load JSON data\n",
    "with open('copper_news.json', 'r') as file:\n",
    "    articles = json.load(file)\n",
    "\n",
    "def clean_text(html_content):\n",
    "    # Remove HTML tags using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text = soup.get_text(separator=' ')\n",
    "    \n",
    "    # Remove URLs, special characters, and numbers\n",
    "    text = re.sub(r'http\\S+|www.\\S+|[^A-Za-z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize, remove stop words, and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text.lower())  # Lowercasing\n",
    "    clean_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "# Preprocess and filter articles\n",
    "preprocessed_articles = []\n",
    "keywords = {'nickel', 'copper'}  # Set of relevant keywords\n",
    "\n",
    "for article in articles:\n",
    "    clean_content = clean_text(article['content'])\n",
    "    # Check if the cleaned content contains any of the keywords\n",
    "    if any(keyword in clean_content for keyword in keywords):\n",
    "        article['content'] = clean_content  # Update with cleaned content\n",
    "        preprocessed_articles.append(article)\n",
    "\n",
    "# Save the preprocessed and filtered articles back to a JSON file\n",
    "with open('preprocessed_articles.json', 'w') as file:\n",
    "    json.dump(preprocessed_articles, file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Example of extracting entities from the first article's content\n",
    "entities = extract_entities(preprocessed_articles[0]['content'])\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Function to add entities to the graph\n",
    "def add_entities_to_graph(entities, graph):\n",
    "    for entity, type in entities:\n",
    "        graph.add_node(entity, type=type)  # Add entity as node\n",
    "\n",
    "# Function to add a simplistic relationship between entities in the same article\n",
    "def add_relationships(graph, entities):\n",
    "    for i, (entity1, type1) in enumerate(entities):\n",
    "        for entity2, type2 in entities[i+1:]:\n",
    "            # This is a simplistic example where we add an edge for every pair of entities in the same article\n",
    "            graph.add_edge(entity1, entity2)\n",
    "\n",
    "# Process each article in your JSON data\n",
    "for article in preprocessed_articles:\n",
    "    entities = extract_entities(article['content'])\n",
    "    add_entities_to_graph(entities, G)\n",
    "    add_relationships(G, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune nodes with a low degree\n",
    "degree_threshold = 2  # Define your own threshold\n",
    "low_degree_nodes = [node for node, degree in G.degree() if degree < degree_threshold]\n",
    "G.remove_nodes_from(low_degree_nodes)\n",
    "\n",
    "# Recalculate layout with increased spacing\n",
    "pos = nx.spring_layout(G, k=1.0, iterations=100)  # You may need to tweak 'k' based on your graph size\n",
    "\n",
    "# Draw the pruned graph\n",
    "plt.figure(figsize=(20, 20))\n",
    "nx.draw(G, pos, with_labels=False, node_size=100, node_color='blue', alpha=0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
